import os, random, helper, plotille, numpy as np, random, file_manipulation, csv
from tabnanny import filename_only
from pickle import TRUE
import termplotlib as tpl
import matplotlib.pyplot as plt
from time import sleep
from progress.bar import Bar
import pandas as pd
import re
import csv
import sys
import yaml
from yaml.loader import SafeLoader
from pathlib import Path

path_to_testdirectory = os.path.dirname(os.path.abspath(sys.argv[1])) 

def read_yaml_file():
    '''reads a csv file with the test parameters specifying:

        Fuzzy_hashing_algorithms: ...
        Path_to_testfiles: ...
        Test_description: ...
    
    returns a  [ list | dict ]
    
    '''
    config_path = sys.argv[1]

    # Open the file and load the file
    with open(config_path) as f:
        data = yaml.load(f, Loader=SafeLoader)
        # print(yaml.dump(data))
    
    return data


def get_rand_bytes(byte_length: int):
    '''generates a random byte sequence of length byte_length

    :param byte_length:
    :return: byte sequence as byt object
    '''

    random_bytes = os.urandom(byte_length) 
       
    return random_bytes

def overwrite_with_chunk(filepath, fragment, fragment_size_percent, target_dir):
    '''takes a random generated byte and a filepath.
    It insers fragment_size_percent (%) into a file (filepath)
    and copies the finished into a specified directory path.

    :param filepath:
    :param fragment:
    :return:
    '''


    filesize = helper.getfilesize(filepath)

    #calculates how long the fragment should be
    fragment_len = int(filesize * (fragment_size_percent * 0.01))

    #choose random position in byte object and cut fragment
    full_fragment_len = len(fragment)
    max_offset = full_fragment_len - fragment_len
    #print(max_offset)
    fragment_start_pos = 0 #random.randrange(0, max_offset)
    fragment_stop_pos = fragment_start_pos + fragment_len

    #choose random offset in file
    offset = file_manipulation.getrandoffset(filepath, fragment_len)

    #cut fragment_len from random pos in file
    fragment_ins = fragment[fragment_start_pos:fragment_stop_pos]

    # if 

    # end is where the chunk ends and the second half begins
    end = offset + fragment_len
    f = open(filepath, "rb")
    first_half = f.read(offset)
    f.seek(end)
    second_half = f.read()
    f.close()

    # merging the three parts
    byt = first_half + fragment_ins + second_half

    # writing bytes to file (filepath)
    filename = helper.get_file_name(filepath)
    f = open("./dataset/anomalies/{}".format(filename), "wb")
    f.write(byt)
    f.close()

    fragment_filepath = target_dir + "/" + filename

    return fragment_filepath

def generate_testdata(amount_of_files, 
                     insertion_mode,
                     patth_to_original_files, 
                     path_to_targets,
                     Fragment_lower_bound,
                     Fragment_upper_bound,
                     Duplication):
    print("stuff")


def create_evaluation_data(fuzzy_hashes, dir_path, dataset_label):
    '''takes every file in a dir_path and hashes it, it collects information about the file as well. 
       The result is a dict with lists with the syntax:

       
       {
        "filename":[filesize, fragment_size, ..., fuzzy_hash_a, fuzzy_hash_b],
        ...
        }
        
       :param fuzzy_hashes: list of fuzzy hashes
       :param dir_path: path to the dir that contains all the files
       :dataset_label: for the this dataset
    '''

    evaluation_data_dict = {}

    for file in os.listdir(dir_path):

        evaluation_data_dict[file] = {}

        file_path = os.path.join(dir_path, file)
        
        #add the file size to dict
        evaluation_data_dict[file]["filesize"] =  helper.getfilesize(file_path)

        #write the fragment_size to dict
        evaluation_data_dict[file]["fragment_size"] = "placeholder"

        #write the filepath to dict
        evaluation_data_dict[file]["filepath"] = file_path

        #label the file (either anomaly or normal)
        if file == "anomaly":
            evaluation_data_dict[file]["label"] = "anomaly"
        else:
            evaluation_data_dict[file]["label"] = dataset_label
        
        for fuzzy_hash in fuzzy_hashes:
            fuzzy_hash_instance = helper.get_algorithm(fuzzy_hash)

            # fuzzy_hash of a file | three hashes do not return hashes as string so write the path to the file instead
            if fuzzy_hash not in ["FBHASH", "MRSHCF", "MRSHV2"]:
                hash_exp = fuzzy_hash_instance.get_hash(file_path)
                evaluation_data_dict[file][fuzzy_hash] = hash_exp
            else:
                evaluation_data_dict[file][fuzzy_hash] = False


    return evaluation_data_dict

def approximate_matching_anomaly_vs_files(evaluation_data_dict: dict):
    ''' takes the "anomaly" file in the dict and compares it to all other files

    '''

    # in the dict all keys after "fragment_size" are hashes ... lets collect them
    available_fuzzy_hashes = []
    keys =  evaluation_data_dict.get('anomaly', {}).keys()
    for elem in keys:
        if elem.isupper():
            available_fuzzy_hashes.append(elem)

    for testfile in evaluation_data_dict: #.items():

        # no sense in comparing anomaly with itself which is why this case is skipped
        if testfile != "anomaly":
            for fuzzy_hash in available_fuzzy_hashes:

                #create an instance of the fuzzy hash 
                fuzzy_hash_instance = helper.get_algorithm(fuzzy_hash)

                # in case that the fuzzy hash does not return a hash as a string (False is set in the dict)
                # the two testfiles are directly compared via their filepaths
                if evaluation_data_dict[testfile][fuzzy_hash] == False:

                    anomaly_filepath = evaluation_data_dict["anomaly"]["filepath"]
                    testfile_path = evaluation_data_dict[testfile]["filepath"]

                    similarity_score = fuzzy_hash_instance.compare_file_against_file(anomaly_filepath, testfile_path)

                # ... in case the fuzzy hash is already in the dict as a string ...
                else:

                    anomaly_hash = evaluation_data_dict["anomaly"][fuzzy_hash]
                    file_hash = evaluation_data_dict[testfile][fuzzy_hash]
                    similarity_score = fuzzy_hash_instance.compare_hash(anomaly_hash, file_hash)
                
                evaluation_data_dict[testfile]["sim_score_{}_anomaly_vs_testfile".format(fuzzy_hash)] = similarity_score

                #-TP : the anomaly was correctly discovered 
                #-FP : the anomaly was incorrectly discovered in a file without any anomaly
                #-FN : the anomaly was not discovered in a file with anomaly
                #-TN : a file without anomaly was correctly determined as beeing normal (similarity score 0)
                filesize = evaluation_data_dict[testfile]["filesize"]

                file_label = evaluation_data_dict[testfile]["label"]

                # when the similarity score is > 0 and the file is labeled as an anomaly (TP)
                if similarity_score > 0 and file_label == "anomaly":
                    #tp_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "tp"
                    #total_tp_size += filesize
                    #total_tp_score += similarity_score

                # when the similarity score is = 0 and the file is labeled as an anomaly (FN)             
                elif similarity_score == 0 and file_label == "anomaly":
                    #fn_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "fn"
                    #total_fn_size += filesize
                    #total_fn_score += similarity_score

                # when the similarity score is > 0 and the file is labeled as normal (FP)
                elif similarity_score > 0 and file_label == "normal":
                    #fp_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "fp"
                    #total_fp_size += filesize
                    #total_fp_score += similarity_score

                # when the similarity score is = 0 and the file is labeled as normal (TN)
                elif similarity_score == 0 and  file_label == "normal":
                    #tn_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "tn"
                    #total_tn_size += filesize
                    #total_tn_score += similarity_score

                else: 
                    print("ERROR, Similarity Score ", fuzzy_hash, " ", 
                        similarity_score," label: " , evaluation_data_dict[testfile]["label"])

    save_path = path_to_testdirectory + '/results.yml'

    if os.path.exists(save_path):
        write_mode = 'a' # append if already exists
    else:
        write_mode = 'w' # make a new file if not
        fle = Path(save_path)
        fle.touch(exist_ok=True)

    write_mode = 'w'	# for debugging purposes this is set to override

    with open(save_path, write_mode) as outfile:
        yaml.dump(evaluation_data_dict, outfile, default_flow_style=False)

    return evaluation_data_dict

def precision_and_recall_calculation(testfile_data_dict):
    '''takes a results file as input and evaluates it based on the labels. Records are:

        -TP : the anomaly was correctly discovered 
        -FP : the anomaly was incorrectly discovered in a file without any anomaly
        -FN : the anomaly was not discovered in a file with anomaly
        -TN : a file without anomaly was correctly determined as beeing normal (similarity score 0)

        - Sizes of TP, FP, FN, TN files
        - distribution of file types
        - avg sim score of TP, FP, FN, TN files should the 
    '''
    
    evaluation_result = {}

    for testfile in testfile_data_dict:

        if testfile != "anomaly":

            #get all fuzzy hashes that a file has been hashed with
            available_fuzzy_hashes = []

            keys =  testfile_data_dict.get(testfile, {}).keys()
            for elem in keys:
                if elem.isupper():
                    available_fuzzy_hashes.append(elem)

            for fuzzy_hash in available_fuzzy_hashes:

                # the mark that deteremines wether a file is a tp, fp , tn, fn
                eval_tag = testfile_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)]
                file_size =  testfile_data_dict[testfile]["filesize"]
                sim_score = testfile_data_dict[testfile]["sim_score_{}_anomaly_vs_testfile".format(fuzzy_hash)]
                file_path = testfile_data_dict[testfile]["filepath"]

                # checks wether the neste dict with the key "fuzzy_hash" has been initialized
                if not fuzzy_hash in evaluation_result:
                    evaluation_result[fuzzy_hash] = {}
                    tags = ["tp","fp","tn","fn"]
                    for tag in tags:
                        #initializing all the dict elements
                        evaluation_result[fuzzy_hash]["{}_ctr".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["total_{}_size".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["total_{}_score".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["avg_{}_size".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["avg_{}_score".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["{}_paths".format(tag)] = []
                    evaluation_result[fuzzy_hash]["accuracy"] = 0
                    evaluation_result[fuzzy_hash]["precision"] = 0     
                    evaluation_result[fuzzy_hash]["recall"] = 0
                    
                # populating the dict
                evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] += 1
                evaluation_result[fuzzy_hash]["total_{}_size".format(eval_tag)] += file_size
                evaluation_result[fuzzy_hash]["total_{}_score".format(eval_tag)]  += sim_score
                evaluation_result[fuzzy_hash]["{}_paths".format(eval_tag)].append(file_path)

                evaluation_result[fuzzy_hash]["avg_{}_size".format(eval_tag)] = \
                    evaluation_result[fuzzy_hash]["total_{}_size".format(eval_tag)] \
                         / evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] \
                        if evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] else 0

                evaluation_result[fuzzy_hash]["avg_{}_score".format(eval_tag)] = \
                    evaluation_result[fuzzy_hash]["total_{}_score".format(eval_tag)] \
                          / evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] \
                        if evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] else 0
                
                true_positive = evaluation_result[fuzzy_hash]["tp_ctr"]
                true_negative = evaluation_result[fuzzy_hash]["tn_ctr"]
                false_positive = evaluation_result[fuzzy_hash]["fp_ctr"]
                false_negative = evaluation_result[fuzzy_hash]["fn_ctr"]

                evaluation_result[fuzzy_hash]["accuracy"] = (true_positive + true_negative) / \
                            (true_positive + true_negative + false_positive + false_negative) \
                                if (true_positive + true_negative + false_positive + false_negative) else 0
                evaluation_result[fuzzy_hash]["precision"] = (true_negative / \
                                                             (true_positive + false_positive)) \
                                                                if (true_positive + false_positive) else 0
                evaluation_result[fuzzy_hash]["recall"] = (true_positive / \
                                                (true_positive + false_negative)) \
                                                if (true_positive + false_negative) else 0


    # has lots of rows in it that are not necessary
    intermediate_dataframe = pd.DataFrame.from_dict(evaluation_result)
    final_dataframe = pd.DataFrame()



    # ...dropping some uninportant rows.
    tags = ["tp","fp","tn","fn"]
    for tag in tags:
        final_dataframe = pd.concat([final_dataframe,
                                                intermediate_dataframe.loc[[
                                                "{}_ctr".format(tag),
                                                "avg_{}_size".format(tag),
                                                "avg_{}_score".format(tag)]]
                                                ])

    final_dataframe = pd.concat([final_dataframe,
                                                intermediate_dataframe.loc[[
                                                "accuracy", 
                                                "precision",
                                                "recall"]]
                                                ])

    # to print evaluation results: yaml.dump(evaluation_result))
    return evaluation_result , final_dataframe     
    


if __name__ == '__main__':
    config = read_yaml_file()
    path_to_testfiles = config["Evaluation"]["Testfile_paths"]
    algorithms = config["Evaluation"]["Fuzzy_hashing_algorithms"]
    label = config["Evaluation"]["Label"]
    test_description = config["Evaluation"]["Test_description"]

    hash_dict = create_evaluation_data(algorithms, path_to_testfiles, label)
    evaluation_data_dict = approximate_matching_anomaly_vs_files(hash_dict)
    evaluation_results, dataframe = precision_and_recall_calculation(evaluation_data_dict)
    print(dataframe)












