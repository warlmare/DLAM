import os, random, helper, plotille, numpy as np, random, file_manipulation, csv

from torch import maximum
from tabnanny import filename_only
from pickle import TRUE
import termplotlib as tpl
import matplotlib.pyplot as plt
from time import sleep
from progress.bar import Bar
import pandas as pd
import re
import csv
import sys
import yaml
from yaml.loader import SafeLoader
from pathlib import Path
import shutil
import pathlib

path_to_testdirectory = os.path.dirname(os.path.abspath(sys.argv[1])) 

def read_yaml_file():
    '''reads a yaml file with the test parameters specifying:

        Fuzzy_hashing_algorithms: ...
        Path_to_testfiles: ...
        Test_description: ...
    
    returns a  [ list | dict ]
    
    '''
    config_path = sys.argv[1]

    # Open the file and load the file
    with open(config_path) as f:
        data = yaml.load(f, Loader=SafeLoader)
        # print(yaml.dump(data))
    
    return data


def get_rand_bytes(byte_length: int):
    '''generates a random byte sequence of length byte_length

    :param byte_length:
    :return: byte sequence as byt object
    '''

    random_bytes = os.urandom(byte_length) 
       
    return random_bytes

def split_list(a_list):
    '''splits a list "a_list" into two list of equal size
    
    '''
    half = len(a_list)//2
    return a_list[:half], a_list[half:]

def overwrite_with_chunk(filepath, fragment, fragment_size_percent, target_dir):
    '''takes a random generated byte and a filepath.
    It insers fragment_size_percent (%) into a file (filepath)
    and copies the finished into a specified directory path.

    :param filepath:
    :param fragment:
    :return:
    '''


    filesize = helper.getfilesize(filepath)

    #calculates how long the fragment should be
    fragment_len = int(filesize * (fragment_size_percent * 0.01))

    #choose random position in byte object and cut fragment
    full_fragment_len = len(fragment)
    max_offset = full_fragment_len - fragment_len
    #print(max_offset)
    fragment_start_pos = 0 #random.randrange(0, max_offset)
    fragment_stop_pos = fragment_start_pos + fragment_len

    #choose random offset in file
    offset = file_manipulation.getrandoffset(filepath, fragment_len)

    #cut fragment_len from random pos in file
    fragment_ins = fragment[fragment_start_pos:fragment_stop_pos]

    # if 

    # end is where the chunk ends and the second half begins
    end = offset + fragment_len
    f = open(filepath, "rb")
    first_half = f.read(offset)
    f.seek(end)
    second_half = f.read()
    f.close()

    # merging the three parts
    byt = first_half + fragment_ins + second_half

    # writing bytes to file (filepath)
    filename = helper.get_file_name(filepath)
    f = open(target_dir + "/" + filename, "wb")
    f.write(byt)
    f.close()

    fragment_filepath = target_dir + "/" + filename

    return fragment_filepath


def get_sample_files(tr_dataset_size, filedirectory, file_type):
    '''selects TRAINING_DATASET_SIZE amount of files of DATASET_FILETYPE

    :param FILETYPE:
    :return: dict with paths to files
    '''
    filepaths_ext = []

    if file_type == "any":
        for file in os.listdir(filedirectory):
            filepath = os.path.join(filedirectory, file)
            filepaths_ext.append(filepath)
    else:
        for file in os.listdir(filedirectory):
            if file.endswith(file_type):
                filepath = os.path.join(filedirectory, file)
                filepaths_ext.append(filepath)


    filesizes = []
    sample = random.sample(filepaths_ext, tr_dataset_size)
    for file in sample:
        filesizes.append(helper.getfilesize(file) / 1000000) # size in MB



    #converting array to numpy array
    filesizes = np.asarray(filesizes    )   #ataset_size = np.sum(filesizes)
    MAX_FILESIZE = np.max(filesizes) * 1000000
    #print("-" * 50, "FILESIZE DISTRIBUTION IN SAMPLE", "-" * 50)
    #print(plotille.histogram(filesizes, x_min=0, x_max=MAX_FILESIZE, y_min=0, y_max=TRAINING_DATASET_SIZE/2, X_label="Size in MB"))
    #print("DATASET SIZE: ", TRAINING_DATASET_SIZE, " MB")
    #print("-" * 131)


    filepaths_all = random.sample(filepaths_ext, tr_dataset_size)
    training_files = random.sample(filepaths_all, tr_dataset_size)
    # stores the difference of the list for training files and all files
    #rest_list = []

    #calculate the difference between the files already picked for trainging and all files 
    # in order to pick only so far un-picked files for testing
    #rest_list = list(set(filepaths_all) - set(training_files))
    #test_files = random.sample(rest_list, test_dataset_size)
    return training_files , int(MAX_FILESIZE)


def generate_testdata(amount_of_files,
                     file_type, #either "any" or "pdf" or something 
                     path_to_original_files,
                     path_where_files_will_be_saved, 
                     split_the_sample_flag, # if "True" than only half of the files will be manipulate
                     generate_a_new_fragment_for_every_file_flag,
                     generate_a_new_fragment_or_use_an_existing_one_flag,
                     path_to_existing_anomaly,
                     shuffle_anomaly_flag,
                     shuffle_anomaly_parts_in_perc, # anomaly will be cut up and every x % percent will be shuffled
                     Fragment_lower_bound,
                     Fragment_upper_bound,
                     duplicate_anomaly_in_file_flag, 
                     duplicate_manipulated_file_flag,
                     duplication_ctr):
    
    #generate dir where files will be saved
    if not os.path.exists(path_where_files_will_be_saved):
        os.mkdir(path_where_files_will_be_saved)
    

    # picks a list of files of a certain filetype the maximum_filesize is important for the anomaly_creation
    if file_type == "random":
        if not os.path.exists(path_to_testdirectory + "/unmanipulated_random_generated_files"):
            os.mkdir(path_to_testdirectory + "/unmanipulated_random_generated_files")
        else:
            shutil.rmtree(path_to_testdirectory + "/unmanipulated_random_generated_files")
            os.mkdir(path_to_testdirectory + "/unmanipulated_random_generated_files")

        rand_filesizes = []
        list_of_future_manipulated_files = []

        for i in range(1,amount_of_files):

            # pick a random number between 500 and 60000 bytes (60KB) and generate as many bytes
            rand_filesize = random.randint(500, 60000)

            # save the filesize, in the end we want to know the biggest filesize
            rand_filesizes.append(rand_filesize)
            
            random_bytes = get_rand_bytes(rand_filesize)

            rand_filename = "random_generated_file_" + str(i)

            rand_filepath = os.path.join(path_to_testdirectory, "unmanipulated_random_generated_files", rand_filename)

            list_of_future_manipulated_files.append(rand_filepath)

            f = open(rand_filepath, "wb")
            f.write(random_bytes)
            f.close()

        maximum_file_size = max(rand_filesizes)

    else:
        list_of_future_manipulated_files, maximum_file_size = get_sample_files(amount_of_files,path_to_original_files,file_type)


    # dividing the dataset into two parts
    if split_the_sample_flag is True:
        anomaly_files_paths, normal_files_paths = split_list(list_of_future_manipulated_files)
    elif split_the_sample_flag is False:
        anomaly_files_paths = list_of_future_manipulated_files

    dataset_files = {}
    dataset_files["anomaly"] = {}
    dataset_files["anomaly"]["filesize"] = maximum_file_size
    dataset_files["anomaly"]["filepath"] = path_where_files_will_be_saved + "/anomaly"

    # generating a anomaly if this fragment is supposed to be the same in all files    
    if generate_a_new_fragment_for_every_file_flag is False:
    
        # if generating a new anomaly is wished otherwise read an exisiting one
        if generate_a_new_fragment_or_use_an_existing_one_flag is True:
            anomaly = get_rand_bytes(maximum_file_size)

            # creating a file with only the randomly generated files
            f = open(path_where_files_will_be_saved + "/anomaly", "wb")
            f.write(anomaly)
            f.close()
        else:
            # if a existing anomaly ought to be inserted
            f = open(path_to_existing_anomaly, "rb")
            anomaly = f.read()
            f.close()
        


    # for the generation of files with anomalies
    ctr = 0
    for path in anomaly_files_paths:
        ctr += 1

        file_name = helper.get_file_name(path)
        dataset_files[file_name] = {}

        file_size = file_manipulation.getfilesize(path)
        dataset_files[file_name]["filesize"] = file_size

        # if the fragment is supposed to be varied than generate a new fragment for every new insertion
        if generate_a_new_fragment_for_every_file_flag is True:
            anomaly = get_rand_bytes(file_size)

        if shuffle_anomaly_flag is True:
            # if the fragment is supposed to be polymorphus i. e. be cut up in equal parts of x that are rearranged

            chunk_size = int(len(anomaly) * shuffle_anomaly_parts_in_perc)
            #list[bytes] is created
            bytes_list = [anomaly[i:i+chunk_size] for i in range(0, len(anomaly), chunk_size)]

            # the list[bytes] of the anomaly are shuffled 
            bytes_list_shuffled = random.sample(bytes_list, len(bytes_list))
            anomaly = b''.join(bytes_list_shuffled)


        #insert fragments into file, th
        fragment_size = random.randint(Fragment_lower_bound,Fragment_upper_bound)
        dataset_files[file_name]["fragment_size"] = fragment_size
        
        # the path where the manipualted file has been saved
        fragment_filepath = overwrite_with_chunk(path, anomaly, fragment_size, path_where_files_will_be_saved)
        
        dataset_files[file_name]["filepath"] = fragment_filepath
        dataset_files[file_name]["label"] = "anomaly"

        print("file generated {}/{}".format(ctr,amount_of_files))



    #create the normal files if wished
    if split_the_sample_flag is True:
        for path in normal_files_paths:
            ctr += 1

            normal_file_name = helper.get_file_name(path)

            dataset_files[normal_file_name] = {}

            dataset_files[normal_file_name]["filesize"] = file_manipulation.getfilesize(path)

            new_filepath = os.path.join(path_where_files_will_be_saved + "/")

            dataset_files[normal_file_name]["filepath"] = shutil.copy(path, new_filepath)

            dataset_files[normal_file_name]["label"] = "normal"

            print("file generated {}/{}".format(ctr,amount_of_files))


    # remove all created testfiles DEBUG
    #shutil.rmtree(path_where_files_will_be_saved)

    return dataset_files         

def hash_data(dataset_files, fuzzy_hashes):
    '''takes every file in a dir_path and hashes it, it collects information about the file as well. 
       The result is a dict with lists with the syntax:

       
       {
        "filename":[filesize, fragment_size, ..., fuzzy_hash_a, fuzzy_hash_b],
        ...
        }
        
       :param fuzzy_hashes: list of fuzzy hashes
       :param dir_path: path to the dir that contains all the files
       :dataset_label: for the this dataset
    '''
    dataset_size = len(dataset_files)
    ctr = 0
    for testfile in dataset_files:
        ctr += 1
        file_path = os.path.relpath(dataset_files[testfile]["filepath"], "./")
        
        for fuzzy_hash in fuzzy_hashes:
            fuzzy_hash_instance = helper.get_algorithm(fuzzy_hash)

            # fuzzy_hash of a file | three hashes do not return hashes as string so write the path to the file instead
            if fuzzy_hash not in ["FBHASH", "MRSHCF", "MRSHV2"]:
                hash_exp = fuzzy_hash_instance.get_hash(file_path)
                dataset_files[testfile][fuzzy_hash] = hash_exp
            else:
                dataset_files[testfile][fuzzy_hash] = False

        print("file hashed {}/{}".format(ctr,dataset_size))

    return dataset_files


def approximate_matching_anomaly_vs_files(evaluation_data_dict: dict):
    ''' takes the "anomaly" file in the dict and compares it to all other files

    '''

    dataset_size = len(evaluation_data_dict)

    # in the dict all keys after "fragment_size" are hashes ... lets collect them
    available_fuzzy_hashes = []
    keys =  evaluation_data_dict.get('anomaly', {}).keys()
    for elem in keys:
        if elem.isupper():
            available_fuzzy_hashes.append(elem)

    ctr = 0
    for testfile in evaluation_data_dict: #.items():
        ctr += 1
        # no sense in comparing anomaly with itself which is why this case is skipped
        if testfile != "anomaly":
            for fuzzy_hash in available_fuzzy_hashes:

                #create an instance of the fuzzy hash 
                fuzzy_hash_instance = helper.get_algorithm(fuzzy_hash)

                # in case that the fuzzy hash does not return a hash as a string (False is set in the dict)
                # the two testfiles are directly compared via their filepaths
                if evaluation_data_dict[testfile][fuzzy_hash] == False:

                    anomaly_filepath = evaluation_data_dict["anomaly"]["filepath"]
                    testfile_path = evaluation_data_dict[testfile]["filepath"]
                    #print(anomaly_filepath, testfile_path)

                    similarity_score = fuzzy_hash_instance.compare_file_against_file(anomaly_filepath, testfile_path)

                # ... in case the fuzzy hash is already in the dict as a string ...
                else:

                    anomaly_hash = evaluation_data_dict["anomaly"][fuzzy_hash]
                    file_hash = evaluation_data_dict[testfile][fuzzy_hash]
                    similarity_score = fuzzy_hash_instance.compare_hash(anomaly_hash, file_hash)
                    
                
                evaluation_data_dict[testfile]["sim_score_{}_anomaly_vs_testfile".format(fuzzy_hash)] = similarity_score

                #-TP : the anomaly was correctly discovered 
                #-FP : the anomaly was incorrectly discovered in a file without any anomaly
                #-FN : the anomaly was not discovered in a file with anomaly
                #-TN : a file without anomaly was correctly determined as beeing normal (similarity score 0)
                filesize = evaluation_data_dict[testfile]["filesize"]

                file_label = evaluation_data_dict[testfile]["label"]

                # when the similarity score is > 0 and the file is labeled as an anomaly (TP)
                if similarity_score > 0 and file_label == "anomaly":
                    #tp_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "tp"
                    #total_tp_size += filesize
                    #total_tp_score += similarity_score

                # when the similarity score is = 0 and the file is labeled as an anomaly (FN)             
                elif similarity_score <= 0 and file_label == "anomaly":
                    #fn_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "fn"
                    #total_fn_size += filesize
                    #total_fn_score += similarity_score

                # when the similarity score is > 0 and the file is labeled as normal (FP)
                elif similarity_score > 0 and file_label == "normal":
                    #fp_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "fp"
                    #total_fp_size += filesize
                    #total_fp_score += similarity_score

                # when the similarity score is = 0 and the file is labeled as normal (TN)
                elif similarity_score <= 0 and  file_label == "normal":
                    #tn_ctr += 1
                    evaluation_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)] = "tn"
                    #total_tn_size += filesize
                    #total_tn_score += similarity_score

                else: 
                    print("ERROR, Similarity Score ", fuzzy_hash, " ", 
                        similarity_score," label: " , evaluation_data_dict[testfile]["label"])

            print("file matched {}/{}".format(ctr,dataset_size))
    
    save_path = path_to_testdirectory + '/labeled_data.yml'

    if os.path.exists(save_path):
        write_mode = 'a' # append if already exists
    else:
        write_mode = 'w' # make a new file if not
        fle = Path(save_path)
        fle.touch(exist_ok=True)

    write_mode = 'w'	# for debugging purposes this is set to override

    with open(save_path, write_mode) as outfile:
        yaml.dump(evaluation_data_dict, outfile, default_flow_style=False)

    return evaluation_data_dict

def precision_and_recall_calculation(testfile_data_dict):
    '''takes a results file as input and evaluates it based on the labels. Records are:

        -TP : the anomaly was correctly discovered 
        -FP : the anomaly was incorrectly discovered in a file without any anomaly
        -FN : the anomaly was not discovered in a file with anomaly
        -TN : a file without anomaly was correctly determined as beeing normal (similarity score 0)

        - Sizes of TP, FP, FN, TN files
        - distribution of file types
        - avg sim score of TP, FP, FN, TN files should the 
    '''
    
    evaluation_result = {}

    for testfile in testfile_data_dict:

        if testfile != "anomaly":

            #get all fuzzy hashes that a file has been hashed with
            available_fuzzy_hashes = []

            keys =  testfile_data_dict.get(testfile, {}).keys()
            for elem in keys:
                if elem.isupper():
                    available_fuzzy_hashes.append(elem)

            for fuzzy_hash in available_fuzzy_hashes:

                # the mark that deteremines wether a file is a tp, fp , tn, fn
                eval_tag = testfile_data_dict[testfile]["p&r_{}_anomaly_vs_testfile".format(fuzzy_hash)]
                file_size =  testfile_data_dict[testfile]["filesize"]
                sim_score = testfile_data_dict[testfile]["sim_score_{}_anomaly_vs_testfile".format(fuzzy_hash)]
                file_path = testfile_data_dict[testfile]["filepath"]
                if 'fragment_size' in testfile_data_dict[testfile].keys():
                    fragment_size = testfile_data_dict[testfile]["fragment_size"]

                # checks wether the neste dict with the key "fuzzy_hash" has been initialized
                if not fuzzy_hash in evaluation_result:
                    evaluation_result[fuzzy_hash] = {}
                    tags = ["tp","fp","tn","fn"]
                    for tag in tags:
                        #initializing all the dict elements
                        evaluation_result[fuzzy_hash]["{}_ctr".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["total_{}_size".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["total_{}_score".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["avg_{}_size".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["avg_{}_score".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["{}_paths".format(tag)] = []
                        evaluation_result[fuzzy_hash]["{}_fragments".format(tag)] = 0
                        evaluation_result[fuzzy_hash]["avg_{}_fragments".format(tag)] = 0
                    evaluation_result[fuzzy_hash]["accuracy"] = 0
                    evaluation_result[fuzzy_hash]["precision"] = 0     
                    evaluation_result[fuzzy_hash]["recall"] = 0
                    
                # populating the dict
                evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] += 1
                evaluation_result[fuzzy_hash]["total_{}_size".format(eval_tag)] += file_size
                evaluation_result[fuzzy_hash]["total_{}_score".format(eval_tag)]  += sim_score
                evaluation_result[fuzzy_hash]["{}_paths".format(eval_tag)].append(file_path)
                evaluation_result[fuzzy_hash]["{}_fragments".format(tag)] += fragment_size
                
                evaluation_result[fuzzy_hash]["avg_{}_size".format(eval_tag)] = \
                    evaluation_result[fuzzy_hash]["total_{}_size".format(eval_tag)] \
                         / evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] \
                        if evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] else 0

                evaluation_result[fuzzy_hash]["avg_{}_score".format(eval_tag)] = \
                    evaluation_result[fuzzy_hash]["total_{}_score".format(eval_tag)] \
                          / evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] \
                        if evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] else 0
                
                evaluation_result[fuzzy_hash]["avg_{}_fragments".format(eval_tag)] = \
                    evaluation_result[fuzzy_hash]["{}_fragments".format(tag)] \
                    / evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] \
                        if evaluation_result[fuzzy_hash]["{}_ctr".format(eval_tag)] else 0

                true_positive = evaluation_result[fuzzy_hash]["tp_ctr"]
                true_negative = evaluation_result[fuzzy_hash]["tn_ctr"]
                false_positive = evaluation_result[fuzzy_hash]["fp_ctr"]
                false_negative = evaluation_result[fuzzy_hash]["fn_ctr"]

                evaluation_result[fuzzy_hash]["accuracy"] = (true_positive + true_negative) / \
                            (true_positive + true_negative + false_positive + false_negative) \
                                if (true_positive + true_negative + false_positive + false_negative) else 0
                evaluation_result[fuzzy_hash]["precision"] = (true_negative / \
                                                             (true_positive + false_positive)) \
                                                                if (true_positive + false_positive) else 0
                evaluation_result[fuzzy_hash]["recall"] = (true_positive / \
                                                (true_positive + false_negative)) \
                                                if (true_positive + false_negative) else 0

    # has lots of rows in it that are not necessary
    intermediate_dataframe = pd.DataFrame.from_dict(evaluation_result)
    final_dataframe = pd.DataFrame()




    # ...dropping some uninportant rows.
    tags = ["tp","fp","tn","fn"]
    for tag in tags:
        final_dataframe = pd.concat([final_dataframe,
                                                intermediate_dataframe.loc[[
                                                "{}_ctr".format(tag),
                                                "avg_{}_size".format(tag),
                                                "avg_{}_score".format(tag),
                                                "avg_{}_fragments".format(tag)]]
                                                ])

    final_dataframe = pd.concat([final_dataframe,
                                                intermediate_dataframe.loc[[
                                                "accuracy", 
                                                "precision",
                                                "recall"]]
                                                ])

    # to print evaluation results: yaml.dump(evaluation_result))
    return evaluation_result , final_dataframe     
    

def config_file_reader():

    config = read_yaml_file()

    testdata_generation_flag = bool(config["Testfilegeneration"]["Active"])
    evaluation_flag = bool(config["Evaluation"]["Active"])

    if testdata_generation_flag == True:
        testdata_protocol = generate_testdata(int(config["Testfilegeneration"]["Amount_of_files"]),
                                                str(config["Testfilegeneration"]["File_type"]), 
                                                str(config["Testfilegeneration"]["Path_to_original_files"]),
                                                str(config["Testfilegeneration"]["Path_where_generated_files_will_be_saved"]), 
                                                bool(config["Testfilegeneration"]["Split_sample_flag"]),
                                                bool(config["Testfilegeneration"]["Generate_a_new_fragment_for_every_file_flag"]),
                                                bool(config["Testfilegeneration"]["Generate_a_new_fragment_or_use_an_existing_one_flag"]),
                                                str(config["Testfilegeneration"]["Path_to_exisiting_anomaly"]),
                                                bool(config["Testfilegeneration"]["Shuffle_anomaly_flag"]),
                                                int(config["Testfilegeneration"]["Shuffle_anomaly_parts_in_perc"]), # anomaly will be cut up and every x % percent will be shuffled
                                                int(config["Testfilegeneration"]["Fragment_size"]["Lower_bound"]),
                                                int(config["Testfilegeneration"]["Fragment_size"]["Upper_bound"]),
                                                bool(config["Testfilegeneration"]["Duplicate_anomaly_in_file_flag"]), 
                                                bool(config["Testfilegeneration"]["Duplicate_manipulated_file_flag"]),
                                                int(config["Testfilegeneration"]["Duplication_Counter"]))
        if evaluation_flag == True:
            algorithms = config["Evaluation"]["Fuzzy_hashing_algorithms"]
            hashed_data = hash_data(testdata_protocol, algorithms)
            approximate_matching_data = approximate_matching_anomaly_vs_files(hashed_data)
            evaluation_results, dataframe = precision_and_recall_calculation(approximate_matching_data)
            save_path = path_to_testdirectory + '/evaluation.yml'
            with open(save_path, "w") as outfile:
                yaml.dump(evaluation_results, outfile, default_flow_style=False)
            print(dataframe)
    elif testdata_generation_flag == False:
        if evaluation_flag == True:
            algorithms = config["Evaluation"]["Fuzzy_hashing_algorithms"]
            labeled_data = config["Evaluation"]["Labeled_data_path"]
            evaluation_results, dataframe = precision_and_recall_calculation(labeled_data)
            save_path = path_to_testdirectory + '/evaluation.yml'
            with open(save_path, "w") as outfile:
                yaml.dump(evaluation_results, outfile, default_flow_style=False)
            print(dataframe)

    shutil.rmtree(str(config["Testfilegeneration"]["Path_where_generated_files_will_be_saved"]))
    shutil.rmtree("evaluation_testcase/unmanipulated_random_generated_files")

if __name__ == '__main__':

    config_file_reader()
    







