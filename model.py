import math
from pickle import FALSE
from pprint import pprint

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import csv
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
import re
from pprint import pprint


#######################
# AUXILIARY FUNCTIONS #
#######################

def read_csv_to_list(csv_file):
    '''reads .csv files with hashes and generates a list

    '''
    with open(csv_file, 'r') as csvfile:
        readcsv = csv.reader(csvfile, delimiter=',')
        flatlist = (list(readcsv))

    return flatlist

def flatten(t):
    '''flattens a list of lists

    '''
    return [item for sublist in t for item in sublist]

def clean_ssdeep_hash(ssdeep_hash):
    '''takes a ssdeep hash and removes all unimportant information from a sseep hash
    like chunksize and the separation-character (:)

    ssdeep formatting: chunksize:chunk:double_chunk
    
    :param ssdeep_hash: string 
    '''

    # remove the first numbers until the :
    hash_without_chunksize = re.sub(r'^.*?:', ':', ssdeep_hash)

    # remove all :'s
    hash_without_sep_char = hash_without_chunksize.replace(":","")

    return hash_without_sep_char

################
# HASH DATASET #
################

# encoding chars for hash -> embeddings
ALL_CHARS = '0abcdefghijklmnopqrstuvwxyz'
ALL_CHARS += 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
ALL_CHARS += '123456789'
ALL_CHARS += '().,-/+=&$?@#!*:;_[]|%‚∏è{}\"\'' + ' ' + '\\'

class HashDataset(Dataset):
    '''HashDataset
    
    '''

    def __init__(self, hashes, labels, all_chars, transform=False):
        """
        .
        """
        super(HashDataset, self).__init__()

        self._vocab_size = len(all_chars)

        self.char_to_int = dict((c, i) for i, c in enumerate(all_chars))
        self.int_to_char = dict((i, c) for i, c in enumerate(all_chars))

        self.hashes = hashes
        self.labels = labels

        self.transform = transform

        self.data_min = 0
        self.data_max = len(all_chars)

    def __len__(self):
        return len(self.hashes)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        inputs = [self.char_to_int[char] for char in self.hashes[idx]]
        label = torch.LongTensor(self.labels[idx])

        if self.transform:
            inputs_scaled = inputs / self.data_max
            return torch.FloatTensor(inputs_scaled), label
        else:
            return torch.LongTensor(inputs), label

    def decode_hash(self, encoded_hash):
        return ''.join(self.int_to_char[_int] for _int in encoded_hash)

    @property
    def vocab_size(self):
        return self._vocab_size



########################
# TRAINING DATA LOADER #
########################

batch_size = 16


# training data consists of 15_000 normal pdf files that have been hashed with ssdeep
training_data_normal = read_csv_to_list("dataset/normal_hashes_training_25000_pdf_tlsh.csv")
# training data consists of 15_000 pdf files that have been hashed mixed (50% size) with a single random 
# generated byte sequence and than hashed with ssdeep
training_data_anom = read_csv_to_list("dataset/anomaly_hashes_training_25000_pdf_tlsh.csv")
training_data_list_unflat = training_data_normal + training_data_anom
training_data_list_unclean = flatten(training_data_list_unflat)

# cleanup all ssdeep hashes
training_data_list = list(map(clean_ssdeep_hash, training_data_list_unclean))

n_training = len(training_data_list)

# anomalie is [0, 1] and normal is [1, 0].
#training_labels = [[1, 0] for i in range(n_training // 2)] + [[0, 1] for i in range(n_training // 2)]

# normal is [0] and  anomalie is [1].
training_labels = [[0]] * (n_training // 2) + [[1]] * (n_training // 2)

train_dataset = HashDataset(
    hashes=training_data_list,
    labels=training_labels,
    all_chars=ALL_CHARS
)


max_hash_length = max([len(hash) for hash in train_dataset.hashes])
dataset_size = len(train_dataset)
vocabulary_size = train_dataset.vocab_size
input_size = vocabulary_size



# Padding and colate function for padding
PADDING_TOKEN = 0 #TODO: maybe change the padding token

def simple_collate_fn(data):
  max_seq_len = max([len(tokens) for tokens, _ in data])
  padded_tokens = [torch.hstack([tokens, 
                                 torch.ones(max_seq_len-len(tokens))*PADDING_TOKEN]) 
                   for tokens, _ in data]
  batch = torch.vstack(padded_tokens)
  labels = torch.vstack([label for _, label in data])
  return batch.int(), labels.long()

#train_data_loader = DataLoader(train_dataset, batch_size, collate_fn=simple_collate_fn, shuffle=True, num_workers=2)
train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2)
train_loader_generator = iter(train_data_loader)


print(train_dataset.__getitem__(1))





#######################
# TRAINING PARAMETERS #
#######################

print_every = 50

seed = 42
torch.manual_seed(seed)
np.random.seed(seed)

device = "cuda" if torch.cuda.is_available() else "cpu"

max_steps =  50000
learning_rate = 1e-4 # 0.01
#batch_size = 16 #256 512
hidden_size = 128

##############
# BERT MODEL #
##############

from transformers import AutoModel, BertConfig, BertGenerationEncoder


class BERTModel(nn.Module):
    # Constructor
    def __init__(self, max_seq_len, hidden_size, vocab_size):

        super(BERTModel, self).__init__()

        # config for a tiny BERT, could be bigger
        config = BertConfig(hidden_size=hidden_size, 
                            intermediate_size=512,  # needs adjusting
                            num_attention_heads=2,  # needs adjusting 
                            num_hidden_layers=2,    # needs adjusting
                            max_position_embeddings=max_seq_len,
                            vocab_size=vocab_size) 

        self.BERT = AutoModel.from_config(config)

        self.predictor = nn.Linear(hidden_size, 2)
        
    def forward(self, x):

        # pass through BERT 
        outputs_per_token = self.BERT(x, return_dict=True)["last_hidden_state"]

        # mean across token dimension 
        output = outputs_per_token.mean(dim=1) 
        
        return self.predictor(output)

########################
# MODEL INITIALIZATION #
########################        

# Model
model = BERTModel(
    max_seq_len=max_hash_length,
    hidden_size=hidden_size,
    vocab_size=vocabulary_size
)
print(model.BERT.config)
model.to(device)

# loss and optimizer
criterion = nn.CrossEntropyLoss() # nn.BCEWithLogitsLoss() 
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

#train_data_loader = DataLoader(train_dataset, batch_size, collate_fn=simple_collate_fn, shuffle=True, num_workers=2)
train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2)
train_loader_generator = iter(train_data_loader)



######################
# MAIN TRAINING LOOP #
###################### 

# main training loop
for step in range(max_steps):
    try:
        # Samples the batch
        batch_inputs, batch_labels = next(train_loader_generator)
    except StopIteration:
        # restart the generator if the previous generator is exhausted.
        train_loader_generator = iter(train_data_loader)
        batch_inputs, batch_labels = next(train_loader_generator)

    batch_inputs = batch_inputs.to(device)
    batch_labels = batch_labels.to(device)

    prediction_logits = model(batch_inputs)
    
    loss = criterion(prediction_logits, batch_labels.view(-1))
    

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % print_every == 0:
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M')}], Step = {step}/{max_steps}, Loss = {loss}")
         
print('Done training.')
torch.save(model, "trained_tiny_bert_model_tlsh.pth")


####################
# TEST DATA LOADER #
####################

# test data consists of 15_000 pdf files that have been hashed mixed (50% size) with a single random 
test_data_normal = read_csv_to_list("dataset/normal_hashes_test_2500_pdf_tlsh.csv")
test_data_anomaly = read_csv_to_list("dataset/anomaly_hashes_test_2500_pdf_tlsh.csv")
test_data_list_unflat = test_data_normal + test_data_anomaly
test_data_list_unclean = flatten(test_data_list_unflat)

# cleanup all ssdeep hashes
test_data_list = list(map(clean_ssdeep_hash, test_data_list_unclean))
n_test = len(test_data_list)
#label data (nomal 0 / anomalie 1)
test_labels = [[0]] * (n_test // 2) + [[1]] * (n_test // 2)
#test_labels = [[1, 0] for i in range(n_test // 2)] + [[0, 1] for i in range(n_test // 2)]

test_dataset = HashDataset(
    hashes=test_data_list,
    labels=test_labels,
    all_chars=ALL_CHARS
)

max_hash_length = max([len(hash) for hash in test_dataset.hashes])
dataset_size = len(test_dataset)
vocabulary_size = test_dataset.vocab_size
input_size = vocabulary_size


#test_data_loader = DataLoader(test_dataset, batch_size, collate_fn=simple_collate_fn ,shuffle=True, num_workers=2)
test_data_loader = DataLoader(test_dataset, batch_size,shuffle=True, num_workers=2)
test_loader_generator = iter(test_data_loader)


########################
# TEST DATA EVALUATION #
########################

model = torch.load("trained_tiny_bert_model_tlsh.pth")
model.eval()



def test(loader_generator, data_loader, dataset_size):

  avg_accuracy = 0 
  ctr = 0
  

  for step in range(dataset_size):
      ctr += 1

      try:
          # Samples the batch
          batch_inputs, batch_labels = next(loader_generator)
      except StopIteration:
          # restart the generator if the previous generator is exhausted.
          loader_generator = iter(data_loader)
          batch_inputs, batch_labels = next(loader_generator)

      batch_inputs = batch_inputs.to(device)
      batch_labels = batch_labels.to(device)

      prediction_logits = model(batch_inputs)


      # flatten the labels of the batch
      batch_labels = torch.flatten(batch_labels)
      
      # get the predictions for the batch
      softmax_predictions = F.softmax(prediction_logits, dim=0)
      argmax_predictions = torch.argmax(softmax_predictions, dim=1)


      # checking the number of equal values in labels and predictions
      correct_predictions = (batch_labels == argmax_predictions).sum().cpu().numpy()

      label_tensor_size = batch_labels.size()
      pred_tensor_size = argmax_predictions.size()


      if label_tensor_size == pred_tensor_size:
        total = pred_tensor_size
        accuracy_per_batch = (100 * correct_predictions / total)
        avg_accuracy += accuracy_per_batch 
        overall_accuracy = ( avg_accuracy / ctr)

      if step % print_every == 0:
        print(f"Step = {step}/{dataset_size}, Accuracy = {overall_accuracy}")



test(train_loader_generator, train_data_loader, 50000)

#parse the result of a model evaluation model(input)
def tensor_float_parser(tensor_str):
  tensor_str = str(tensor_str)
  try:
    pat = r'.*?\[(.*)].*'            
    match = re.search(pat, tensor_str)
    target_str = match.group(1)  
    arr = np.array([float(i) for i in re.findall(r"[-+]?\d*\.\d+|\d+", target_str)])
  except:# when the tensor_str is a label with [1. , 0.]
    arr = np.array([int(s) for s in tensor_str.split() if s.isdigit()])
  return arr



# normal is [0] and anomalie is [1].
# raw prediction logit syntax: [normal, anomaly]
def check_prediction_for_single_input(single_instance):
  '''takes an input and a label and checks wether the 
  model predicts correctly 
  '''
  custom_loader = DataLoader(single_instance,1)
  input, label = iter(custom_loader)
  input = input.to(device)
  prob = F.softmax(model(input), dim=1)

  # turns the softmax prediction into a readable result
  parsed_prediction= tensor_float_parser(prob)
  print(parsed_prediction)
  if parsed_prediction[0] < parsed_prediction[1]:
    prediction_string = "normal"
  elif parsed_prediction[0] > parsed_prediction[1]:
    prediction_string = "anomalous"
  # case that the prediction is incoconclusive
  elif parsed_prediction[0] == parsed_prediction[1]:
    result_dict = {"correct " : False,
                   "original label" : tensor_float_parser(label)[0],
                   "prediction" : "indecisive"} 
    pprint(result_dict, width=1)
    return 

  # parses the label 
  arr_lab = tensor_float_parser(label)
  if arr_lab[0] == 0: 
    label_str = "normal"
  elif arr_lab[0] == 1:
    label_str = "anomalous"
  
  # check if prediction is correct
  if label_str == prediction_string:
    prediction_correctness = "correct"
  else:
    prediction_correctness = "false"
  result_dict = {"correct " : prediction_correctness,
                 "original label" : label_str,
                 "prediction" : prediction_string} 
  
  pprint(result_dict, width=1)




def test_prediction_bool(single_instance):
  '''takes an input and a label and checks wether the 
  model predicts correctly 
  '''
  custom_loader = DataLoader(single_instance,1)
  input, label = iter(custom_loader)
  input = input.to(device)
  prob = F.softmax(model(input), dim=1)

  # turns the softmax prediction into a readable result
  parsed_prediction= tensor_float_parser(prob)
  if parsed_prediction[0] < parsed_prediction[1]:
    prediction_string = "normal"
  elif parsed_prediction[0] > parsed_prediction[1]:
    prediction_string = "anomalous"
  elif parsed_prediction[0] == parsed_prediction[1]:
    return False

  # parses the label 
  arr_lab = tensor_float_parser(label)
  if arr_lab[0] == 0: 
    label_str = "normal"
  elif arr_lab[0] == 1:
    label_str = "anomalous"
  
  # check if prediction is correct
  if label_str == prediction_string:
    return True
  else:
    return False
