
import os, random, helper, plotille, numpy as np, random, file_manipulation, csv
import termplotlib as tpl
import matplotlib.pyplot as plt
from time import sleep
from progress.bar import Bar
import pandas as pd

import csv


DATASET_FILETYPE = "pdf"
DATASET_SIZE = 30_000
SAMPLE_FILES_PATH = "../govdocs/all_files"
HASHING_ALGORITHM = "TLSH"
FRAGMENT_PERCENTAGE = 50


def get_sample_files():
    '''selects DATASET_SIZE amount of files of DATASET_FILETYPE

    :param FILETYPE:
    :return: dict with paths to files
    '''
    filepaths_ext = []

    for file in os.listdir(SAMPLE_FILES_PATH):
        if file.endswith(DATASET_FILETYPE):
            filepath = os.path.join(SAMPLE_FILES_PATH, file)
            filepaths_ext.append(filepath)


    filesizes = []
    sample = random.sample(filepaths_ext, DATASET_SIZE)
    for file in sample:
        filesizes.append(helper.getfilesize(file) / 1000000) # size in MB



    #converting array to numpy array
    filesizes = np.asarray(filesizes)
    dataset_size = np.sum(filesizes)
    MAX_FILESIZE = np.max(filesizes) * 1000000
    #print("-" * 50, "FILESIZE DISTRIBUTION IN SAMPLE", "-" * 50)
    #print(plotille.histogram(filesizes, x_min=0, x_max=MAX_FILESIZE, y_min=0, y_max=DATASET_SIZE/2, X_label="Size in MB"))
    #print("DATASET SIZE: ", dataset_size, " MB")
    #print("-" * 131)


    filepaths = random.sample(filepaths_ext, DATASET_SIZE)
    return filepaths, int(MAX_FILESIZE)


def get_rand_bytes(byte_length: int):
    '''generates a random byte sequence of length byte_length

    :param byte_length:
    :return: byte sequence as byt object
    '''

    random_bytes = random.randbytes(byte_length)
    return random_bytes


def overwrite_with_chunk(filepath, fragment, fragment_size_percent):
    '''takes a random generated byte and a filepath.
    It insers fragment_size_percent (%) into a file (filepath)
    and copies the finished into a specified directory path.

    :param filepath:
    :param fragment:
    :return:
    '''


    filesize = helper.getfilesize(filepath)

    #calculates how long the fragment should be
    fragment_len = int(filesize * (fragment_size_percent * 0.01))

    #choose random position in byte object and cut fragment
    full_fragment_len = len(fragment)
    max_offset = full_fragment_len - fragment_len
    #print(max_offset)
    fragment_start_pos = random.randrange(0, max_offset)
    fragment_stop_pos = fragment_start_pos + fragment_len

    #choose random offset in file
    offset = file_manipulation.getrandoffset(filepath, fragment_len)

    #cut fragment_len from random pos in file
    fragment_ins = fragment[fragment_start_pos:fragment_stop_pos]

    # end is where the chunk ends and the second half begins
    end = offset + fragment_len
    f = open(filepath, "rb")
    first_half = f.read(offset)
    f.seek(end)
    second_half = f.read()
    f.close()

    # merging the three parts
    byt = first_half + fragment_ins + second_half

    # writing bytes to file (filepath)
    filename = helper.get_file_name(filepath)
    f = open("./dataset/anomalies/{}".format(filename), "wb")
    f.write(byt)
    f.close()

    fragment_filepath = "./dataset/anomalies/{}".format(filename)

    return fragment_filepath


def split_list(a_list):
    half = len(a_list)//2
    return a_list[:half], a_list[half:]

def generate_dataset():

    #generating a sample_set of files
    sample_files_paths, max_filesize = get_sample_files()

    #dividing the dataset into two parts
    list_a, normal_files = split_list(sample_files_paths)

    #generating a random sequence of bytes
    anomaly = get_rand_bytes(max_filesize)

    # creating a file with only the randomly generated files
    f = open("./dataset/anomaly", "wb")
    f.write(anomaly)
    f.close()

    anomaly_files = []

    ctr = 1
    for path in list_a:
        #insert fragments into file
        fragment_filepath = overwrite_with_chunk(path, anomaly, FRAGMENT_PERCENTAGE)
        anomaly_files.append(fragment_filepath)
        #print("DATASET GENERATION: {}/{}".format(ctr,DATASET_SIZE))
        ctr += 1

    return anomaly_files, normal_files

def generate_hashes_from_dataset(dataset_paths):

    hashes = []

    algorithm_instance = helper.get_algorithm(HASHING_ALGORITHM)

    for path in dataset_paths:
        sample_hash = algorithm_instance.get_hash(path)
        hashes.append(sample_hash)

    return hashes

def list_to_csv(list_x, filename):
    with open(filename, 'w', newline='') as myfile:
        wr = csv.writer(myfile)
        wr.writerow(list_x)




if __name__ == '__main__':

    anomaly_files, normal_files = generate_dataset()
    anomaly_hashes = generate_hashes_from_dataset(anomaly_files)
    normal_hashes = generate_hashes_from_dataset(normal_files)
    list_to_csv(anomaly_hashes, "dataset/anomaly_hashes_training_15k_pdf_tlsh.csv")
    list_to_csv(normal_hashes, "dataset/normal_hashes_training_15k_pdf_tlsh.csv")










